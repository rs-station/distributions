{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"rs-distributions","text":"<p>Table of Contents</p> <ul> <li>Installation</li> <li>Distributions</li> <li>Modules</li> <li>License</li> </ul> <p>rs-distributions provides statistical tools which are helpful for structural biologists who wish to model their data using variational inference. </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install rs-distributions\n</code></pre>"},{"location":"#distributions","title":"Distributions","text":"<p><code>rs_distributions.distributions</code> provides learnable distributions that are important in structural biology.  These distributions follow the conventions in <code>torch.dist</code>.  Here's a small example of distribution matching between a learnable distribution, <code>q</code>, and a target distribion, <code>p</code>.  The example works by minimizing the Kullback-Leibler divergence between <code>q</code> and <code>p</code> using gradients calculated by the implicit reparameterization method. </p> <p><pre><code>import torch\nfrom rs_distributions import distributions as rsd\n\ntarget_loc = 4.\ntarget_scale = 2.\n\nloc_initial_guess = 10.\nscale_initial_guess  = 3.\n\nloc = torch.tensor(loc_initial_guess, requires_grad=True)\n\nscale_transform = torch.distributions.transform_to(\n    rsd.FoldedNormal.arg_constraints['scale']\n)\nscale_initial_guess = scale_transform.inv(\n    torch.tensor(scale_initial_guess)\n)\nunconstrained_scale = torch.tensor(\n    torch.tensor(scale_initial_guess),\n    requires_grad=True\n)\n\np = rsd.FoldedNormal(\n    target_loc,\n    target_scale,\n)\n\nopt = torch.optim.Adam([loc, unconstrained_scale])\n\nsteps = 10_000\nnum_samples = 100\nfor i in range(steps):\n    opt.zero_grad()\n    scale = scale_transform(unconstrained_scale)\n    q = rsd.FoldedNormal(loc, scale)\n    z = q.sample((num_samples,))\n    kl_div = q.log_prob(z) - p.log_prob(z)\n    kl_div = kl_div.mean()\n    kl_div.backward()\n    opt.step()\n</code></pre> This example uses the folded normal distribution which is important in X-ray crystallography. </p>"},{"location":"#modules","title":"Modules","text":"<p>Working with PyTorch distributions can be a little verbose.  So in addition to the <code>torch.distributions</code> style implementation, we provide <code>DistributionModule</code> classes which enable learnable distributions with automatic bijections in less code.  These <code>DistributionModule</code> classes are subclasses of <code>torch.nn.Module</code>.  They automatically instantiate problem parameters as <code>TransformedParameter</code> modules following the constraints in the distribution definition. In the following example, a <code>FoldedNormal</code> <code>DistributionModule</code> is instantiated with an initial location and scale and trained to match a target distribution. </p> <pre><code>from rs_distributions import modules as rsm\nimport torch\n\nloc_init = 10.\nscale_init = 5.\n\nq = rsm.FoldedNormal(loc_init, scale_init)\np = torch.distributions.HalfNormal(1.)\n\nopt = torch.optim.Adam(q.parameters())\n\nsteps = 10_000\nnum_samples = 256\nfor i in range(steps):\n    opt.zero_grad()\n    z = q.rsample((num_samples,))\n    kl = (q.log_prob(z) - p.log_prob(z)).mean()\n    kl.backward()\n    opt.step()\n</code></pre>"},{"location":"#license","title":"License","text":"<p><code>rs-distributions</code> is distributed under the terms of the MIT license.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>rs_distributions<ul> <li>distributions</li> <li>modules</li> <li>transforms</li> </ul> </li> </ul>"},{"location":"reference/rs_distributions/","title":"Api","text":""},{"location":"reference/rs_distributions/distributions/","title":"Distributions","text":""},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.FoldedNormal","title":"<code>FoldedNormal</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>Folded Normal distribution class</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>float or Tensor</code> <p>location parameter of the distribution</p> required <code>scale</code> <code>float or Tensor</code> <p>scale parameter of the distribution (must be positive)</p> required <code>validate_args</code> <code>bool</code> <p>Whether to validate the arguments of the distribution.</p> <code>None</code> <code>var_thresh</code> <code>int</code> <p>Threshold for switching between the Folded Normal variance formula and the Normal variance</p> <code>5</code> Source code in <code>src/rs_distributions/distributions/folded_normal.py</code> <pre><code>class FoldedNormal(dist.Distribution):\n    \"\"\"\n    Folded Normal distribution class\n\n    Args:\n        loc (float or Tensor): location parameter of the distribution\n        scale (float or Tensor): scale parameter of the distribution (must be positive)\n        validate_args (bool, optional): Whether to validate the arguments of the distribution.\n        Default is None.\n        var_thresh (int): Threshold for switching between the Folded Normal variance formula and the Normal variance\n    \"\"\"\n\n    arg_constraints = {\"loc\": dist.constraints.real, \"scale\": dist.constraints.positive}\n    support = torch.distributions.constraints.nonnegative\n\n    def __init__(self, loc, scale, var_thresh: int = 5, validate_args=None):\n        self.loc, self.scale = torch.distributions.utils.broadcast_all(loc, scale)\n        batch_shape = self.loc.shape\n        super().__init__(batch_shape, validate_args=validate_args)\n        self._irsample = NormalIRSample().apply\n        self.var_thresh = var_thresh\n\n    def log_prob(self, value):\n        \"\"\"\n        Compute the log-probability of the given values under the Folded Normal distribution\n\n        Args:\n            value (Tensor): The values at which to evaluate the log-probability\n\n        Returns:\n            Tensor: The log-probabilities of the given values\n        \"\"\"\n        if self._validate_args:\n            self._validate_sample(value)\n        loc = self.loc\n        scale = self.scale\n        log_prob = torch.logaddexp(\n            dist.Normal(loc, scale).log_prob(value),\n            dist.Normal(-loc, scale).log_prob(value),\n        )\n        return log_prob\n\n    def sample(self, sample_shape=torch.Size()):\n        \"\"\"\n        Generate random samples from the Folded Normal distribution\n\n        Args:\n            sample_shape (torch.Size, optional): The shape of the samples to generate.\n            Default is an empty shape\n\n        Returns:\n            Tensor: The generated random samples\n        \"\"\"\n        shape = self._extended_shape(sample_shape)\n        eps = torch.randn(shape, dtype=self.loc.dtype, device=self.loc.device)\n        samples = torch.abs(eps * self.scale + self.loc)\n\n        return samples\n\n    @property\n    def mean(self):\n        \"\"\"\n        Compute the mean of the Folded Normal distribution\n\n        Returns:\n            Tensor: The mean of the distribution.\n        \"\"\"\n        loc = self.loc\n        scale = self.scale\n        return scale * torch.sqrt(torch.tensor(2.0) / torch.pi) * torch.exp(\n            -0.5 * (loc / scale) ** 2\n        ) + loc * (1 - 2 * dist.Normal(0, 1).cdf(-loc / scale))\n\n    @property\n    def variance(self):\n        \"\"\"\n        Compute the variance of the Folded Normal distribution\n\n        Returns:\n            Tensor: The variance of the distribution\n        \"\"\"\n        loc = self.loc\n        scale = self.scale\n        mean = self.mean\n\n        a = loc / scale\n        var = torch.empty_like(loc)\n        large = a &gt; self.var_thresh\n        small = ~large\n\n        # use Normal variance when loc/scale &gt; var_thresh\n        if large.any():\n            var[large] = scale[large] ** 2\n\n        if small.any():\n            var[small] = loc[small] ** 2 + scale[small] ** 2 - mean[small] ** 2\n\n        return var\n\n    def cdf(self, value):\n        \"\"\"\n        Args:\n            value (Tensor): The values at which to evaluate the CDF\n\n        Returns:\n            Tensor: The CDF values at the given values\n        \"\"\"\n        if self._validate_args:\n            self._validate_sample(value)\n        value = torch.as_tensor(value, dtype=self.loc.dtype, device=self.loc.device)\n        # return dist.Normal(loc, scale).cdf(value) - dist.Normal(-loc, scale).cdf(-value)\n        return 0.5 * (\n            torch.erf((value + self.loc) / (self.scale * np.sqrt(2.0)))\n            + torch.erf((value - self.loc) / (self.scale * np.sqrt(2.0)))\n        )\n\n    def dcdfdmu(self, value):\n        return torch.exp(\n            dist.Normal(-self.loc, self.scale).log_prob(value)\n        ) - torch.exp(dist.Normal(self.loc, self.scale).log_prob(value))\n\n    def dcdfdsigma(self, value):\n        A = (-(value + self.loc) / self.scale) * torch.exp(\n            dist.Normal(-self.loc, self.scale).log_prob(value)\n        )\n        B = (-(value - self.loc) / self.scale) * torch.exp(\n            dist.Normal(self.loc, self.scale).log_prob(value)\n        )\n        return A + B\n\n    def pdf(self, value):\n        return torch.exp(self.log_prob(value))\n\n    def rsample(self, sample_shape=torch.Size()):\n        \"\"\"\n        Generate differentiable random samples from the Folded Normal distribution.\n        Gradients are implemented using implicit reparameterization (https://arxiv.org/abs/1805.08498).\n\n        Args:\n            sample_shape (torch.Size, optional): The shape of the samples to generate.\n            Default is an empty shape\n\n        Returns:\n            Tensor: The generated random samples\n        \"\"\"\n        samples = self.sample(sample_shape)\n        # F = self.cdf(samples)\n        q = self.pdf(samples)\n        dFdmu = self.dcdfdmu(samples)\n        dFdsigma = self.dcdfdsigma(samples)\n        samples.requires_grad_(True)\n        return self._irsample(self.loc, self.scale, samples, dFdmu, dFdsigma, q)\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.FoldedNormal.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Compute the mean of the Folded Normal distribution</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The mean of the distribution.</p>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.FoldedNormal.variance","title":"<code>variance</code>  <code>property</code>","text":"<p>Compute the variance of the Folded Normal distribution</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The variance of the distribution</p>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.FoldedNormal.cdf","title":"<code>cdf(value)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The values at which to evaluate the CDF</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The CDF values at the given values</p> Source code in <code>src/rs_distributions/distributions/folded_normal.py</code> <pre><code>def cdf(self, value):\n    \"\"\"\n    Args:\n        value (Tensor): The values at which to evaluate the CDF\n\n    Returns:\n        Tensor: The CDF values at the given values\n    \"\"\"\n    if self._validate_args:\n        self._validate_sample(value)\n    value = torch.as_tensor(value, dtype=self.loc.dtype, device=self.loc.device)\n    # return dist.Normal(loc, scale).cdf(value) - dist.Normal(-loc, scale).cdf(-value)\n    return 0.5 * (\n        torch.erf((value + self.loc) / (self.scale * np.sqrt(2.0)))\n        + torch.erf((value - self.loc) / (self.scale * np.sqrt(2.0)))\n    )\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.FoldedNormal.log_prob","title":"<code>log_prob(value)</code>","text":"<p>Compute the log-probability of the given values under the Folded Normal distribution</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The values at which to evaluate the log-probability</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The log-probabilities of the given values</p> Source code in <code>src/rs_distributions/distributions/folded_normal.py</code> <pre><code>def log_prob(self, value):\n    \"\"\"\n    Compute the log-probability of the given values under the Folded Normal distribution\n\n    Args:\n        value (Tensor): The values at which to evaluate the log-probability\n\n    Returns:\n        Tensor: The log-probabilities of the given values\n    \"\"\"\n    if self._validate_args:\n        self._validate_sample(value)\n    loc = self.loc\n    scale = self.scale\n    log_prob = torch.logaddexp(\n        dist.Normal(loc, scale).log_prob(value),\n        dist.Normal(-loc, scale).log_prob(value),\n    )\n    return log_prob\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.FoldedNormal.rsample","title":"<code>rsample(sample_shape=torch.Size())</code>","text":"<p>Generate differentiable random samples from the Folded Normal distribution. Gradients are implemented using implicit reparameterization (https://arxiv.org/abs/1805.08498).</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Size</code> <p>The shape of the samples to generate.</p> <code>Size()</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The generated random samples</p> Source code in <code>src/rs_distributions/distributions/folded_normal.py</code> <pre><code>def rsample(self, sample_shape=torch.Size()):\n    \"\"\"\n    Generate differentiable random samples from the Folded Normal distribution.\n    Gradients are implemented using implicit reparameterization (https://arxiv.org/abs/1805.08498).\n\n    Args:\n        sample_shape (torch.Size, optional): The shape of the samples to generate.\n        Default is an empty shape\n\n    Returns:\n        Tensor: The generated random samples\n    \"\"\"\n    samples = self.sample(sample_shape)\n    # F = self.cdf(samples)\n    q = self.pdf(samples)\n    dFdmu = self.dcdfdmu(samples)\n    dFdsigma = self.dcdfdsigma(samples)\n    samples.requires_grad_(True)\n    return self._irsample(self.loc, self.scale, samples, dFdmu, dFdsigma, q)\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.FoldedNormal.sample","title":"<code>sample(sample_shape=torch.Size())</code>","text":"<p>Generate random samples from the Folded Normal distribution</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Size</code> <p>The shape of the samples to generate.</p> <code>Size()</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The generated random samples</p> Source code in <code>src/rs_distributions/distributions/folded_normal.py</code> <pre><code>def sample(self, sample_shape=torch.Size()):\n    \"\"\"\n    Generate random samples from the Folded Normal distribution\n\n    Args:\n        sample_shape (torch.Size, optional): The shape of the samples to generate.\n        Default is an empty shape\n\n    Returns:\n        Tensor: The generated random samples\n    \"\"\"\n    shape = self._extended_shape(sample_shape)\n    eps = torch.randn(shape, dtype=self.loc.dtype, device=self.loc.device)\n    samples = torch.abs(eps * self.scale + self.loc)\n\n    return samples\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.Rice","title":"<code>Rice</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>The Rice distribution is useful for modeling acentric structure factor amplitudes in X-ray crystallography. It is the amplitude distribution corresponding to a bivariate normal in the complex plane.</p> <p><pre><code>x ~ MVN([\u03bd, 0], \u03c3I)\ny = sqrt(x[0] * x[0] + x[1] * x[1])\n</code></pre> The parameters \u03bd and \u03c3 represent the location and standard deviation of an isotropic, bivariate normal. If x is drawn from the normal with location [\u03bd, 0i] and covariance, <pre><code>| \u03c3   0 |\n| 0  \u03c3i |\n</code></pre> the distribution of amplitudes, <code>y = sqrt(x * conjugate(x))</code>, follows a Rician distribution.</p> <p>Parameters:</p> Name Type Description Default <code>nu</code> <code>float or Tensor</code> <p>location parameter of the underlying bivariate normal</p> required <code>sigma</code> <code>float or Tensor</code> <p>standard deviation of the underlying bivariate normal (must be positive)</p> required <code>validate_args</code> <code>bool</code> <p>Whether to validate the arguments of the distribution.</p> <code>None</code> Source code in <code>src/rs_distributions/distributions/rice.py</code> <pre><code>class Rice(dist.Distribution):\n    \"\"\"\n    The Rice distribution is useful for modeling acentric structure factor amplitudes in\n    X-ray crystallography. It is the amplitude distribution corresponding to a bivariate\n    normal in the complex plane.\n\n    ```\n    x ~ MVN([\u03bd, 0], \u03c3I)\n    y = sqrt(x[0] * x[0] + x[1] * x[1])\n    ```\n    The parameters \u03bd and \u03c3 represent the location and standard deviation of an isotropic, bivariate normal.\n    If x is drawn from the normal with location [\u03bd, 0i] and covariance,\n    ```\n    | \u03c3   0 |\n    | 0  \u03c3i |\n    ```\n    the distribution of amplitudes, `y = sqrt(x * conjugate(x))`, follows a Rician distribution.\n\n    Args:\n        nu (float or Tensor): location parameter of the underlying bivariate normal\n        sigma (float or Tensor): standard deviation of the underlying bivariate normal (must be positive)\n        validate_args (bool, optional): Whether to validate the arguments of the distribution.\n        Default is None.\n    \"\"\"\n\n    arg_constraints = {\n        \"nu\": dist.constraints.nonnegative,\n        \"sigma\": dist.constraints.positive,\n    }\n    support = torch.distributions.constraints.nonnegative\n\n    def __init__(self, nu, sigma, validate_args=None):\n        self.nu, self.sigma = torch.distributions.utils.broadcast_all(nu, sigma)\n        batch_shape = self.nu.shape\n        super().__init__(batch_shape, validate_args=validate_args)\n        self._irsample = RiceIRSample().apply\n\n    def log_prob(self, value):\n        \"\"\"\n        Compute the log-probability of the given values under the Rice distribution\n\n        ```\n        Rice(x | nu, sigma) = \\\n            x * sigma**-2 * exp(-0.5 * (x**2 + nu**2) * sigma ** -2) * I_0(x * nu * sigma **-2)\n        ```\n\n        Args:\n            value (Tensor): The values at which to evaluate the log-probability\n\n        Returns:\n            Tensor: The log-probabilities of the given values\n        \"\"\"\n        if self._validate_args:\n            self._validate_sample(value)\n        nu, sigma = self.nu, self.sigma\n        x = value\n        log_prob = (\n            torch.log(x)\n            - 2.0 * torch.log(sigma)\n            - 0.5 * torch.square((x - nu) / sigma)\n            + torch.log(torch.special.i0e(nu * x / (sigma * sigma)))\n        )\n        return log_prob\n\n    def sample(self, sample_shape=torch.Size()):\n        \"\"\"\n        Generate random samples from the Rice distribution\n\n        Args:\n            sample_shape (torch.Size, optional): The shape of the samples to generate.\n            Default is an empty shape\n\n        Returns:\n            Tensor: The generated random samples\n        \"\"\"\n        shape = self._extended_shape(sample_shape)\n        nu, sigma = self.nu, self.sigma\n        nu = nu.expand(shape)\n        sigma = sigma.expand(shape)\n        with torch.no_grad():\n            A = torch.normal(nu, sigma)\n            B = torch.normal(torch.zeros_like(nu), sigma)\n            z = torch.sqrt(A * A + B * B)\n            return z\n\n    @property\n    def mean(self):\n        \"\"\"\n        Compute the mean of the Rice distribution\n\n        Returns:\n            Tensor: The mean of the distribution.\n        \"\"\"\n        sigma = self.sigma\n        nu = self.nu\n\n        x = -0.5 * torch.square(nu / sigma)\n        L = (1.0 - x) * torch.special.i0e(-0.5 * x) - x * torch.special.i1e(-0.5 * x)\n        mean = sigma * math.sqrt(math.pi / 2.0) * L\n        return mean\n\n    @property\n    def variance(self):\n        \"\"\"\n        Compute the variance of the Rice distribution\n\n        Returns:\n            Tensor: The variance of the distribution\n        \"\"\"\n        nu, sigma = self.nu, self.sigma\n        return 2 * sigma * sigma + nu * nu - torch.square(self.mean)\n\n    def cdf(self, value):\n        \"\"\"\n        Args:\n            value (Tensor): The values at which to evaluate the CDF\n\n        Returns:\n            Tensor: The CDF values at the given values\n        \"\"\"\n        raise NotImplementedError(\"The CDF is not implemented\")\n\n    def _grad_z(self, samples):\n        \"\"\"\n        Return the gradient of samples from this distribution\n\n        Args:\n            samples (Tensor): samples from this distribution\n\n        Returns:\n            dnu: gradient with respect to the loc parameter, nu\n            dsigma: gradient with respect to the underlying normal's scale parameter, sigma\n        \"\"\"\n        z = samples\n        nu, sigma = self.nu, self.sigma\n        ab = z * nu / (sigma * sigma)\n        dnu = torch.special.i1e(ab) / torch.special.i0e(ab)  # == i1(ab)/i0(ab)\n        dsigma = (z - nu * dnu) / sigma\n        return dnu, dsigma\n\n    def pdf(self, value):\n        return torch.exp(self.log_prob(value))\n\n    def rsample(self, sample_shape=torch.Size()):\n        \"\"\"\n        Generate differentiable random samples from the Rice distribution.\n        Gradients are implemented using implicit reparameterization (https://arxiv.org/abs/1805.08498).\n\n        Args:\n            sample_shape (torch.Size, optional): The shape of the samples to generate.\n            Default is an empty shape\n\n        Returns:\n            Tensor: The generated random samples\n        \"\"\"\n        samples = self.sample(sample_shape)\n        dnu, dsigma = self._grad_z(samples)\n        samples.requires_grad_(True)\n        return self._irsample(self.nu, self.sigma, samples, dnu, dsigma)\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.Rice.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Compute the mean of the Rice distribution</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The mean of the distribution.</p>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.Rice.variance","title":"<code>variance</code>  <code>property</code>","text":"<p>Compute the variance of the Rice distribution</p> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The variance of the distribution</p>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.Rice.cdf","title":"<code>cdf(value)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The values at which to evaluate the CDF</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The CDF values at the given values</p> Source code in <code>src/rs_distributions/distributions/rice.py</code> <pre><code>def cdf(self, value):\n    \"\"\"\n    Args:\n        value (Tensor): The values at which to evaluate the CDF\n\n    Returns:\n        Tensor: The CDF values at the given values\n    \"\"\"\n    raise NotImplementedError(\"The CDF is not implemented\")\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.Rice.log_prob","title":"<code>log_prob(value)</code>","text":"<p>Compute the log-probability of the given values under the Rice distribution</p> <pre><code>Rice(x | nu, sigma) =             x * sigma**-2 * exp(-0.5 * (x**2 + nu**2) * sigma ** -2) * I_0(x * nu * sigma **-2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The values at which to evaluate the log-probability</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The log-probabilities of the given values</p> Source code in <code>src/rs_distributions/distributions/rice.py</code> <pre><code>def log_prob(self, value):\n    \"\"\"\n    Compute the log-probability of the given values under the Rice distribution\n\n    ```\n    Rice(x | nu, sigma) = \\\n        x * sigma**-2 * exp(-0.5 * (x**2 + nu**2) * sigma ** -2) * I_0(x * nu * sigma **-2)\n    ```\n\n    Args:\n        value (Tensor): The values at which to evaluate the log-probability\n\n    Returns:\n        Tensor: The log-probabilities of the given values\n    \"\"\"\n    if self._validate_args:\n        self._validate_sample(value)\n    nu, sigma = self.nu, self.sigma\n    x = value\n    log_prob = (\n        torch.log(x)\n        - 2.0 * torch.log(sigma)\n        - 0.5 * torch.square((x - nu) / sigma)\n        + torch.log(torch.special.i0e(nu * x / (sigma * sigma)))\n    )\n    return log_prob\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.Rice.rsample","title":"<code>rsample(sample_shape=torch.Size())</code>","text":"<p>Generate differentiable random samples from the Rice distribution. Gradients are implemented using implicit reparameterization (https://arxiv.org/abs/1805.08498).</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Size</code> <p>The shape of the samples to generate.</p> <code>Size()</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The generated random samples</p> Source code in <code>src/rs_distributions/distributions/rice.py</code> <pre><code>def rsample(self, sample_shape=torch.Size()):\n    \"\"\"\n    Generate differentiable random samples from the Rice distribution.\n    Gradients are implemented using implicit reparameterization (https://arxiv.org/abs/1805.08498).\n\n    Args:\n        sample_shape (torch.Size, optional): The shape of the samples to generate.\n        Default is an empty shape\n\n    Returns:\n        Tensor: The generated random samples\n    \"\"\"\n    samples = self.sample(sample_shape)\n    dnu, dsigma = self._grad_z(samples)\n    samples.requires_grad_(True)\n    return self._irsample(self.nu, self.sigma, samples, dnu, dsigma)\n</code></pre>"},{"location":"reference/rs_distributions/distributions/#rs_distributions.distributions.Rice.sample","title":"<code>sample(sample_shape=torch.Size())</code>","text":"<p>Generate random samples from the Rice distribution</p> <p>Parameters:</p> Name Type Description Default <code>sample_shape</code> <code>Size</code> <p>The shape of the samples to generate.</p> <code>Size()</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The generated random samples</p> Source code in <code>src/rs_distributions/distributions/rice.py</code> <pre><code>def sample(self, sample_shape=torch.Size()):\n    \"\"\"\n    Generate random samples from the Rice distribution\n\n    Args:\n        sample_shape (torch.Size, optional): The shape of the samples to generate.\n        Default is an empty shape\n\n    Returns:\n        Tensor: The generated random samples\n    \"\"\"\n    shape = self._extended_shape(sample_shape)\n    nu, sigma = self.nu, self.sigma\n    nu = nu.expand(shape)\n    sigma = sigma.expand(shape)\n    with torch.no_grad():\n        A = torch.normal(nu, sigma)\n        B = torch.normal(torch.zeros_like(nu), sigma)\n        z = torch.sqrt(A * A + B * B)\n        return z\n</code></pre>"},{"location":"reference/rs_distributions/modules/","title":"Modules","text":""},{"location":"reference/rs_distributions/modules/#rs_distributions.modules.DistributionModule","title":"<code>DistributionModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for constructing learnable distributions. This subclass of <code>torch.nn.Module</code> acts like a <code>torch.distributions.Distribution</code> object with learnable <code>torch.nn.Parameter</code> attributes. It works by lazily constructing distributions as needed. Here is a simple example of distribution matching using learnable distributions with reparameterized gradients.</p> <pre><code>from rs_distributions import modules as rsm\nimport torch\n\nq = rsm.FoldedNormal(10., 5.)\np = torch.distributions.HalfNormal(1.)\n\nopt = torch.optim.Adam(q.parameters())\n\nsteps = 10_000\nnum_samples = 256\nfor i in range(steps):\n    opt.zero_grad()\n    z = q.rsample((num_samples,))\n    kl = (q.log_prob(z) - p.log_prob(z)).mean()\n    kl.backward()\n    opt.step()\n</code></pre> Source code in <code>src/rs_distributions/modules/distribution.py</code> <pre><code>class DistributionModule(torch.nn.Module):\n    \"\"\"\n    Base class for constructing learnable distributions.\n    This subclass of `torch.nn.Module` acts like a `torch.distributions.Distribution`\n    object with learnable `torch.nn.Parameter` attributes.\n    It works by lazily constructing distributions as needed.\n    Here is a simple example of distribution matching using learnable distributions with reparameterized gradients.\n\n    ```python\n    from rs_distributions import modules as rsm\n    import torch\n\n    q = rsm.FoldedNormal(10., 5.)\n    p = torch.distributions.HalfNormal(1.)\n\n    opt = torch.optim.Adam(q.parameters())\n\n    steps = 10_000\n    num_samples = 256\n    for i in range(steps):\n        opt.zero_grad()\n        z = q.rsample((num_samples,))\n        kl = (q.log_prob(z) - p.log_prob(z)).mean()\n        kl.backward()\n        opt.step()\n    ```\n    \"\"\"\n\n    distribution_class = torch.distributions.Distribution\n    __doc__ = distribution_class.__doc__\n    arg_constraints = distribution_class.arg_constraints\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        sig = signature(self.distribution_class)\n        bargs = sig.bind(*args, **kwargs)\n        bargs.apply_defaults()\n        for arg in self.distribution_class.arg_constraints:\n            param = bargs.arguments.pop(arg)\n            param = self._constrain_arg_if_needed(arg, param)\n            setattr(self, f\"_transformed_{arg}\", param)\n        self._extra_args = bargs.arguments\n\n    def __repr__(self):\n        rstring = super().__repr__().split(\"\\n\")[1:]\n        rstring = [str(self.distribution_class) + \" DistributionModule(\"] + rstring\n        return \"\\n\".join(rstring)\n\n    def _distribution(self):\n        kwargs = {\n            k: self._realize_parameter(getattr(self, f\"_transformed_{k}\"))\n            for k in self.distribution_class.arg_constraints\n        }\n        kwargs.update(self._extra_args)\n        return self.distribution_class(**kwargs)\n\n    def _constrain_arg_if_needed(self, name, value):\n        if isinstance(value, TransformedParameter):\n            return value\n        cons = self.distribution_class.arg_constraints[name]\n        if cons == torch.distributions.constraints.dependent:\n            transform = torch.distributions.AffineTransform(0.0, 1.0)\n        else:\n            transform = torch.distributions.constraint_registry.transform_to(cons)\n        return TransformedParameter(value, transform)\n\n    @staticmethod\n    def _realize_parameter(param):\n        if isinstance(param, TransformedParameter):\n            return param()\n        return param\n\n    def __getattr__(self, name: str):\n        if name in self.distribution_class.arg_constraints or hasattr(\n            self.distribution_class, name\n        ):\n            q = self._distribution()\n            return getattr(q, name)\n        return super().__getattr__(name)\n\n    @staticmethod\n    def _extract_distributions(*modules, base_class=torch.distributions.Distribution):\n        \"\"\"\n        extract all torch.distributions.Distribution subclasses from a module(s)\n        into a dict {name: cls}\n        \"\"\"\n        d = {}\n        for module in modules:\n            for k in module.__all__:\n                distribution_class = getattr(module, k)\n                if not hasattr(distribution_class, \"arg_constraints\"):\n                    continue\n                if not hasattr(distribution_class.arg_constraints, \"items\"):\n                    continue\n                if issubclass(distribution_class, base_class):\n                    d[k] = distribution_class\n        return d\n\n    def __init_subclass__(cls, /, distribution_class, **kwargs):\n        super().__init_subclass__(**kwargs)\n        update_wrapper(\n            cls.__init__,\n            distribution_class.__init__,\n        )\n        cls.distribution_class = distribution_class\n        cls.arg_constraints = distribution_class.arg_constraints\n        cls.__doc__ = distribution_class.__doc__\n</code></pre>"},{"location":"reference/rs_distributions/modules/#rs_distributions.modules.TransformedParameter","title":"<code>TransformedParameter</code>","text":"<p>               Bases: <code>Module</code></p> <p>A <code>torch.nn.Module</code> subclass representing a constrained variabled.</p> Source code in <code>src/rs_distributions/modules/transformed_parameter.py</code> <pre><code>class TransformedParameter(torch.nn.Module):\n    \"\"\"\n    A `torch.nn.Module` subclass representing a constrained variabled.\n    \"\"\"\n\n    def __init__(self, value, transform):\n        \"\"\"\n        Args:\n            value : Tensor\n                The initial value of this learnable parameter\n            transform : torch.distributions.Transform\n                A transform instance which is applied to the underlying, unconstrained value\n        \"\"\"\n        super().__init__()\n        value = torch.as_tensor(value)  # support floats\n        if isinstance(value, torch.nn.Parameter):\n            self._value = value\n            value.data = transform.inv(value)\n        else:\n            self._value = torch.nn.Parameter(transform.inv(value))\n        self.transform = transform\n\n    def forward(self):\n        return self.transform(self._value)\n</code></pre>"},{"location":"reference/rs_distributions/modules/#rs_distributions.modules.TransformedParameter.__init__","title":"<code>__init__(value, transform)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>value </code> <p>Tensor The initial value of this learnable parameter</p> required <code>transform </code> <p>torch.distributions.Transform A transform instance which is applied to the underlying, unconstrained value</p> required Source code in <code>src/rs_distributions/modules/transformed_parameter.py</code> <pre><code>def __init__(self, value, transform):\n    \"\"\"\n    Args:\n        value : Tensor\n            The initial value of this learnable parameter\n        transform : torch.distributions.Transform\n            A transform instance which is applied to the underlying, unconstrained value\n    \"\"\"\n    super().__init__()\n    value = torch.as_tensor(value)  # support floats\n    if isinstance(value, torch.nn.Parameter):\n        self._value = value\n        value.data = transform.inv(value)\n    else:\n        self._value = torch.nn.Parameter(transform.inv(value))\n    self.transform = transform\n</code></pre>"},{"location":"reference/rs_distributions/transforms/","title":"Transforms","text":""},{"location":"reference/rs_distributions/transforms/#rs_distributions.transforms.DiagTransform","title":"<code>DiagTransform</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Applies transformation to the diagonal of a square matrix</p> Source code in <code>src/rs_distributions/transforms/fill_scale_tril.py</code> <pre><code>class DiagTransform(Transform):\n    \"\"\"\n    Applies transformation to the diagonal of a square matrix\n    \"\"\"\n\n    def __init__(self, diag_transform):\n        super().__init__()\n        self.diag_transform = diag_transform\n\n    @property\n    def domain(self):\n        return self.diag_transform.domain\n\n    @property\n    def codomain(self):\n        return self.diag_transform.codomain\n\n    @property\n    def bijective(self):\n        return self.diag_transform.bijective\n\n    def _call(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input matrix\n        Returns\n            torch.Tensor: Transformed matrix\n        \"\"\"\n        diagonal = x.diagonal(dim1=-2, dim2=-1)\n        transformed_diagonal = self.diag_transform(diagonal)\n        result = x.diagonal_scatter(transformed_diagonal, dim1=-2, dim2=-1)\n\n        return result\n\n    def _inverse(self, y):\n        diagonal = y.diagonal(dim1=-2, dim2=-1)\n        result = y.diagonal_scatter(self.diag_transform.inv(diagonal), dim1=-2, dim2=-1)\n        return result\n\n    def log_abs_det_jacobian(self, x, y):\n        diagonal = x.diagonal(dim1=-2, dim2=-1)\n        return self.diag_transform.log_abs_det_jacobian(diagonal, y)\n</code></pre>"},{"location":"reference/rs_distributions/transforms/#rs_distributions.transforms.FillScaleTriL","title":"<code>FillScaleTriL</code>","text":"<p>               Bases: <code>ComposeTransform</code></p> <p>A <code>ComposeTransform</code> that reshapes a real-valued vector into a lower triangular matrix. The diagonal of the matrix is transformed with <code>diag_transform</code>.</p> Source code in <code>src/rs_distributions/transforms/fill_scale_tril.py</code> <pre><code>class FillScaleTriL(ComposeTransform):\n    \"\"\"\n    A `ComposeTransform` that reshapes a real-valued vector into a lower triangular matrix.\n    The diagonal of the matrix is transformed with `diag_transform`.\n    \"\"\"\n\n    def __init__(self, diag_transform=None):\n        if diag_transform is None:\n            diag_transform = torch.distributions.ComposeTransform(\n                (\n                    SoftplusTransform(),\n                    AffineTransform(1e-5, 1.0),\n                )\n            )\n        super().__init__([FillTriL(), DiagTransform(diag_transform=diag_transform)])\n        self.diag_transform = diag_transform\n\n    @property\n    def bijective(self):\n        return True\n\n    def log_abs_det_jacobian(self, x, y):\n        x = FillTriL()._call(x)\n        diagonal = x.diagonal(dim1=-2, dim2=-1)\n        return self.diag_transform.log_abs_det_jacobian(diagonal, diagonal)\n\n    @staticmethod\n    def params_size(event_size):\n        \"\"\"\n        Returns the number of parameters required to create an n-by-n lower triangular matrix, which is given by n*(n+1)//2\n\n        Args:\n            event_size (int): size of event\n        Returns:\n            int: Number of parameters needed\n\n        \"\"\"\n        return event_size * (event_size + 1) // 2\n</code></pre>"},{"location":"reference/rs_distributions/transforms/#rs_distributions.transforms.FillScaleTriL.params_size","title":"<code>params_size(event_size)</code>  <code>staticmethod</code>","text":"<p>Returns the number of parameters required to create an n-by-n lower triangular matrix, which is given by n*(n+1)//2</p> <p>Parameters:</p> Name Type Description Default <code>event_size</code> <code>int</code> <p>size of event</p> required <p>Returns:     int: Number of parameters needed</p> Source code in <code>src/rs_distributions/transforms/fill_scale_tril.py</code> <pre><code>@staticmethod\ndef params_size(event_size):\n    \"\"\"\n    Returns the number of parameters required to create an n-by-n lower triangular matrix, which is given by n*(n+1)//2\n\n    Args:\n        event_size (int): size of event\n    Returns:\n        int: Number of parameters needed\n\n    \"\"\"\n    return event_size * (event_size + 1) // 2\n</code></pre>"},{"location":"reference/rs_distributions/transforms/#rs_distributions.transforms.FillTriL","title":"<code>FillTriL</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Transform for converting a real-valued vector into a lower triangular matrix</p> Source code in <code>src/rs_distributions/transforms/fill_scale_tril.py</code> <pre><code>class FillTriL(Transform):\n    \"\"\"\n    Transform for converting a real-valued vector into a lower triangular matrix\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def domain(self):\n        return constraints.real_vector\n\n    @property\n    def codomain(self):\n        return constraints.lower_triangular\n\n    @property\n    def bijective(self):\n        return True\n\n    def _call(self, x):\n        \"\"\"\n        Converts real-valued vector to lower triangular matrix.\n\n        Args:\n            x (torch.Tensor): input real-valued vector\n        Returns:\n            torch.Tensor: Lower triangular matrix\n        \"\"\"\n\n        return vec_to_tril_matrix(x)\n\n    def _inverse(self, y):\n        return tril_matrix_to_vec(y)\n\n    def log_abs_det_jacobian(self, x, y):\n        batch_shape = x.shape[:-1]\n        return torch.zeros(batch_shape, dtype=x.dtype, device=x.device)\n</code></pre>"}]}